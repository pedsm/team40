<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<h2 id="lecture-13-memory-management">Lecture 13: Memory Management</h2>
<p>This lecture contains:</p>
<ul>
<li>Introduction to memory management</li>
<li><strong>Modelling</strong> of multi-programming</li>
<li>Memory management based on <strong>fixed partitioning</strong></li>
</ul>
<h3 id="memory-hierarchies">Memory Hierarchies</h3>
<p>The lecturer expresses often that he likes to think of a machine's memory as a linear array of memory. You can think of all memory as being stored on this array. This is an abstract way of thinking about it but can help understanding. There are several different types of computer memory, and these can be arranged into a hierarchy. This hierarchy and the reasons behind it influence how memory is managed in a system. Here is a detailed view of the hierarchy (don't need to memorise).</p>
<div class="figure">
<img src="img/memoryHierarchy.png" alt="Memory Hierarchy" />
<p class="caption">Memory Hierarchy</p>
</div>
<ul>
<li><strong>Registers</strong> and <strong>cache</strong> - These are fast because they are located close to the CPU. There are sub-levels of cache memory called L1, L2, L3. These have slightly increasing speeds.</li>
<li>Main memory - This is basically <strong>RAM</strong>. Main memory isn't as fast because it takes a significant time for the CPU to access it. This time period is known as latency.</li>
<li><strong>Disks</strong> - Includes things like Hard Drives. Used for important information. Is much <em>cheaper</em> than other types.</li>
</ul>
<p>The operating system's job is to provide a memory abstraction. But how does it do this?</p>
<h3 id="os-responsibilities">OS Responsibilities</h3>
<ul>
<li>The OS needs to <strong>allocate</strong> memory, and then keep track of whether or not the memory is being used. If it is not being used, it must <strong>deallocate</strong> this memory.</li>
<li>It needs to <em>control access</em> when multiprogramming is applied. In other words it needs to prioritise certain tasks.</li>
<li>It needs to be able to transparently move data from memory to disk and vice versa.</li>
</ul>
<h3 id="memory-models">Memory models</h3>
<p>There are two models which approach memory allocation in different ways. They are:</p>
<ol style="list-style-type: decimal">
<li><strong>Contiguous</strong> memory management models</li>
<li><strong>Non-contiguous</strong> memory management models</li>
</ol>
<p>The easiest way to understand this is through a graphical example:</p>
<div class="figure">
<img src="img/memoryModels.png" alt="Memory Models" />
<p class="caption">Memory Models</p>
</div>
<p>As you can see, the <em>contiguous</em> process is unchanged when stored in memory, whereas the <em>non-contiguous</em> memory is stored in segments (think linked-lists).</p>
<h3 id="contiguous-approaches">Contiguous Approaches</h3>
<h4 id="mono-programming">Mono-programming</h4>
<p>Mono-programming is when there is one single partition for all user processes. For Mono-programming, a fixed region of memory is allocated to the OS/kernel, and the rest is reserved for a single user process. The OS can be thought of as another process, and so you can think of it as two processes. Just remember there is only one <em>user</em> process. This is how MS-DOS worked. Here is a graphical example for how the memory is split:</p>
<div class="figure">
<img src="img/monoModel.png" alt="Mono-programming memory split" />
<p class="caption">Mono-programming memory split</p>
</div>
<ul>
<li>This used contiguous memory allocation. Since there are no other processes, there is no use splitting up the memory and using a non-contiguous approach. Remember that only one process is being fulfilled at any time.</li>
<li>This one process is therefore allocated across the entire memory space, and this process is always located in the same address space. This is why there is no <em>address translation</em></li>
<li>The implementation is very simple, since the memory location is always known. No protection between different processes is required.</li>
<li>You can use <strong>overlays</strong> to enable the programmer to use more memory than available. Basically need to hack the program (probably don't need to know about this but just know it used to be OP).</li>
</ul>
<p>There are however, several disadvantages of this approach, some of which may be obvious.</p>
<ul>
<li>As mentioned earlier, there is one block of memory, with some being given to the OS, and the rest being used for a user process. This sharing could cause problems. The user process could have direct access to the physical memory, and if something screws up it could end up having access to the OS memory.</li>
<li>You can't multitask and so this approach is very outdated.</li>
<li>There is low utilisation of hardware resources such as CPU, I/O devices- this can make processes slow.</li>
</ul>
<p>Despite its limitations, the ease of memory access means that some modern appliances still use mono-programming. An example is a washing machine, where multiple processes aren't desirable (maybe not for Japanese machines). It is kind-of possible to simulate a multi-programming environment on a mono-programming machine. This can be achieved through <strong>swapping</strong>. This is the process of <em>swapping</em> a process out to the disk and loading a new one. These context switches, however, can be time consuming and so aren't really worth.</p>
<h4 id="multi-programming">Multi-programming</h4>
<p>He does some maths to prove that CPU utilisation is higher on multi-programming machines, and that CPU utilisation increases as the number of processes increase. I doubt we'll get tested on this but check the lecture if you're interested.</p>
<h3 id="partitioning">Partitioning</h3>
<p>I will now go over partitioning in the context of memory allocation for multi-programming systems.</p>
<h4 id="fixed-partitions-of-equal-size">Fixed Partitions of Equal Size</h4>
<p>This is when you split the main memory into <em>static, contiguous and equal sized partitions</em>(chunks). These have a fixed size and location. Any process can take any partition providing it is large enough. The actual memory allocation is simple since you don't need to worry about how much memory to give a process. The OS only has to keep track of which partitions are being used. Conceptually, this method is garbage for the following reasons:</p>
<ul>
<li>You may have a small process which doesn't need much memory, using a large partition for a lot of memory. This means you could potentially waste a lot of memory. This is known as having <strong>Overlays</strong>.</li>
<li>If a process is too big for one of the partitions, you won't be able to run it. This is because the partitions are <em>static</em>, meaning they can't be dynamically adjusted.</li>
</ul>
<h4 id="fixed-partitions-of-non-equal-size">Fixed Partitions of Non-Equal Size</h4>
<p>This is when you would partition the memory into non-equal sized partitions instead. For example instead of having 5 partitions with 5M each, you have 5 different partitions with 3M, 4M, 5M, 6M, 7M respectively. This reduces <em>internal fragmentation</em> since you are wasting less memory. Note that the partitions are still static and fixed size. This method also has its drawbacks:</p>
<ul>
<li>More work needs to be done for the allocation of processes to the partitions.</li>
<li>It assumes that a program knows how much memory it needs. You could have a program that uses dynamic memory allocation and so deciding which partition to put it inside would be a pain in the ass.</li>
</ul>
<p>The following diagram describes two ways by which the OS can allocate processes in such a partition.</p>
<div class="figure">
<img src="img/partitionAllocation.png" alt="Non-Equal Partition Allocation" />
<p class="caption">Non-Equal Partition Allocation</p>
</div>
<ul>
<li><p>Diagram (a) shows a method where there is one process queue per partition. This means that each partition has a queue of processes that require at least that amount of memory. In other words, it is first come first served for the processes (you can probably already detect problems with this). One example would be if there were an abundance of processes that required just one partition. This means that only one partition would be busy and so memory usage would be inefficient.</p></li>
<li><p>Diagram (b) uses a single-queue. This means the computer takes each process as it comes and puts it into the smallest available partition. This partially solves the problem mentioned with for process queue (which is that you may be wasting several partitions). This method however, results in <em>increased internal fragrmentation</em>. This is because a 2M process may end up in a 6M partition if that is the smallest partition that is free.</p></li>
</ul>
<h2 id="lecture-14---memory-management">Lecture 14 - Memory Management</h2>
<p>Topics covered in this lecture include:</p>
<ul>
<li>Code <strong>relocation</strong> and <strong>protection</strong></li>
<li><strong>Dynamic partitioning</strong></li>
<li><strong>Swapping</strong></li>
<li><strong>Managing free/occupied</strong> memory</li>
</ul>
<h3 id="introduction-to-logical-addresses">Introduction to logical addresses</h3>
<p>The lecture starts with a simple code example.</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="ot">#include &lt;stdio.h&gt;</span>

<span class="dt">int</span> iVar = <span class="dv">0</span>;
<span class="dt">int</span> main() {
    <span class="dt">int</span> i = <span class="dv">0</span>;
    <span class="kw">while</span>(i &lt; <span class="dv">10</span>) {
    iVar++;
    sleep(<span class="dv">2</span>);
    printf(<span class="st">&quot;Address: %xl Value: %d </span><span class="ch">\n</span><span class="st">&quot;</span>, &amp;iVar, iVar);
    i++;
    }
}</code></pre></div>
<p>This code is simply printing the address and value of an incrementing variable. The lecturer asks if you run this program twice at the same time, whether or not the same address will be printed. The answer is yes, and it is here we are introduced to the concept of <strong>logical addresses</strong>. The <strong>logical address</strong> is basically the address given to the item at compile time, i.e. when the program is executed. This logical address may be different from the <strong>physical address</strong> which can be due to the operation of an <em>address translator</em> or <em>mapping function</em>. It is the OS's job to then translate this logical address into a physical address. This physical address that the OS assigns is likely to be different every time the program is run. So if the program were to be run at the same time, there would be no confusion or interference between variables. Remember that the <em>logical address</em> is assigned at compile time, so the variable in the code above will print the same address as long as it isn't recompiled.</p>
<h3 id="relocation-and-protection-principles">Relocation and protection principles</h3>
<ul>
<li><p>Remember how I said that the OS needs to translate the logical address into physical memory? This is known as <strong>relocation</strong>. The <em>relocation</em> must be solved by the OS in a way that allows for processes to be run at <em>changing memory locations</em>.</p></li>
<li><p><strong>Protection</strong> is what is enforced if you have two or more programs running at the same time. Linking it back to the code example, it ensures that the variables don't get stored in the same physical memory slots (its not just RNG).</p></li>
</ul>
<div class="figure">
<img src="img/addressRelocation.png" alt="Address Relocation" />
<p class="caption">Address Relocation</p>
</div>
<p>This diagram can be a bit confusing so I will split it up into parts.</p>
<ul>
<li>Process A is the process that needs to be allocated into memory.</li>
<li>That weird thing with <em>MMU</em> written inside it represents the <em>Memory Management Unit</em>. This is basically the part of the OS that converts the logical address into the physical address.</li>
<li>In an array, the <em>Offset</em> is the distance from the beginning of the array.</li>
<li>The top partition is dedicated to the OS/kernel.</li>
</ul>
<p>The <em>Memory Management Unit</em> uses the <em>Offset</em> value to calculate where the nearest available memory is, and then uses this knowledge to place the process A into this partition.</p>
<p>They have another slide to remind us of the difference between logical and physical memory addresses. It seems important so make sure you understand it. They are two separate things. The logical address is what is seen by the process. This logical address space is then mapped onto the machines physical address space by the OS.</p>
<h3 id="relocation-and-protection-approaches">Relocation and Protection Approaches</h3>
<p>You may be wondering about <em>when</em> the <strong>relocation</strong> needs to occur. There are three approaches to this:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Static</strong> relocation at <strong>compile time</strong> - This is impractical for multi-processing systems because you don't know which partitions in main memory are free or not. It is therefore a shitty YOLO approach and so I doubt it will come up in the test.</p></li>
<li><p><strong>Dynamic</strong> relocation at <strong>load time</strong> - This is similar to the address relocation figure shown earlier where an <em>offset</em> is added to every logical address to account for its physical location in memory. This method however, doesn't account for <strong>swapping</strong> (will be explained later). For this reason, the loading process will be slow if relocation is done at load time.</p></li>
<li><p><strong>Dynamic</strong> relocation at <strong>runtime</strong> - This is the fastest method but is more difficult and requires special hardware support.</p></li>
</ol>
<h3 id="dynamic-runtime-relocation">Dynamic Runtime Relocation</h3>
<p>In order to achieve relocation at runtime, it relies on using two <em>registers</em>. These registers are special-purpose and so are only used for these tasks. They are:</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>base register</strong> - Stores the <em>start address</em> of the partition. In other words, it uses the <em>offset</em> value mentioned previously. At <em>runtime</em>, a physical address is generated by adding this base register to the logical address.</p></li>
<li><p>The <strong>limit register</strong> - Stores the required size of the partition. At runtime, the resulting physical address is <em>compared</em> against the value in the limit register. This acts as a form of <strong>protection</strong>, as it ensures that the process is getting the correct amount of memory it needs and no less.</p></li>
</ol>
<h3 id="dynamic-partitioning">Dynamic Partitioning</h3>
<p>I have no idea why it took this long for them to tell us about this. The clue is in the name. It dynamically assigns a <em>variable number of partitions</em> of which the <em>size</em> and <em>starting address</em> can change over time. The process gets allocated an exact amount of <strong>contiguous memory</strong> and therefore removes the problem of internal fragmentation. Reminder: contiguous memory is memory stored in one whole block and not split up. This concept is likely to come up in the exam so be sure to remember that. Note that the exact memory requirements may not be known in advance. For example the <em>heap and stack</em> can grow dynamically. This is why a good OS practice is to allocate a bit extra memory to account for programs requiring more memory without having to use dynamic partitioning right away. This is good practice since many programs end up using more memory as time goes on.</p>
<h3 id="swapping">Swapping</h3>
<p>So sure, <em>dynamic partitioning</em> is OP. We get it...you vape. But what if a process is taking up loads of memory but isn't actually being used? For example what happens if its waiting for input/output or some shit like that? This is where <strong>swapping</strong> comes into play.</p>
<p>Swapping holds some of the processes on the <strong>drive</strong> and <strong>shuttles</strong> (fancy word for swap I think) processes between the drive and main memory when required. The reasons for swapping are include:</p>
<ul>
<li>Some processes only run occasionally but may still take up valuable memory.</li>
<li>If you have more processes than partitions- swapping increases efficiency a lot.</li>
<li>A process's memory requirements may have changed - you can use swapping as a method for re-allocating that memory.</li>
<li>If the <em>total</em> amount of memory required for the process exceeds the available memory- then the OS can outplay it with swapping.</li>
</ul>
<p>NB: Swapping IS time consuming, but still saves time overall.</p>
<h3 id="external-fragmentation">External Fragmentation</h3>
<p>Whilst <em>swapping</em> and <em>dynamic partitioning</em> can remove the problem of <em>internal fragmentation</em>, swapping can cause a new problem called <em>External Fragmentation</em>. This when swapping a process out of memory will create &quot;a hole&quot;. A new process may be either too large or too small for this gap that is left behind. If it is too small, memory is wasted in the form of an <em>unused block</em>. If a process is too large, the process may be unable to find a slot even if the total spare memory capacity is there. This leads to a decrease in efficiency. It IS possible to <em>compact</em> the memory to remove the holes and this is known as dynamic relocation. This is however, a very slow process and isn't worth a lot of the time.</p>
<h3 id="allocation-structures">Allocation Structures</h3>
<p>Man! All of this memory management shit sounds really complicated! If only there was a data structure that would allow me to not only keep track of available memory, but also provide a way to quickly allocate processes to available memory slots! No Pedro, the answer does not lie in memory.js, but instead lies in <em>Linked Lists</em>. The following image shows what such a list would look like:</p>
<div class="figure">
<img src="img/linkedList.png" alt="Using Linked Lists for Memory Management" />
<p class="caption">Using Linked Lists for Memory Management</p>
</div>
<p>There is a struct for each partition that contains a flag for whether or not it is free. It also contains data items such as the start of the memory block, and the size (same shit stored in the <em>base</em> and <em>limit</em> registers). This makes the allocation of processes to unused blocks a lot easier.</p>
<p>An alternative data structure we can use are <strong>bitmaps</strong>. First, memory is split into blocks. Do not get this confused with partitions. This is simply the bitmap representation of the physical memory. E.g. one bitmap may have 10,000 blocks with an <em>allocation unit</em> of 10kb each. A bit map is set up so that each bit is 0 if the memory block is free and 1 if the block is used. You can find a hole of a specified memory amount by finding the right number of adjacent bits set to 0. Here is a good bitmap representation I found:</p>
<div class="figure">
<img src="img/bitmap.png" alt="Bitmap memory allocation" />
<p class="caption">Bitmap memory allocation</p>
</div>
<p>In this minimalistic example, the bitmap is split into 40 blocks. This is a representation of the whole physical memory in fixed sized blocks. This concept wasn't clear to me initially hence the repetition.</p>
<p>Bitmaps have advantages and disadvantages. Disadvantages include:</p>
<ul>
<li><p>If the bitmap <em>allocation units</em> (blocks) are a small amount of memory, it means that the overall bitmap is going to be large (will contain more 1's and 0's). This can potentially make bitmaps very slow to process.</p></li>
<li><p>Now consider the other scenario where the <em>allocation unit</em> is a large amount of memory. This can cause <em>internal fragmentation</em> because the bitmap may declare memory to be used which isn't actually being used</p></li>
<li><p>Since linked lists can link <em>free</em> nodes together, it is therefore faster to find a free <em>block</em> (hole) with linked lists</p></li>
</ul>
<p>It is these disadvantages why bitmaps are much less common than linked lists in this context.</p>
<p>In terms of advantages, the actual process of filling the hole is easier and faster with bitmaps because you just need to change the corresponding values from 0 to 1. Also I'm pretty sure bitmaps take up less space. The lecturer said that the advantages and disadvantages of linked lists/ bitmaps could come up in the test which is why I did some extra research on it. So make sure you understand the key differences.</p>
<h2 id="lecture-15---memory-management">Lecture 15 - Memory Management</h2>
<p>The goals for this lecture are:</p>
<ul>
<li><strong>Dynamic partitioning management</strong> with Linked lists</li>
<li><strong>Non-contiguous approaches</strong></li>
<li><strong>Paging</strong>, page tables, address translation</li>
</ul>
<h3 id="allocating-available-memory---algorithms">Allocating Available Memory - Algorithms</h3>
<h4 id="first-fit-algorithm">First Fit Algorithm</h4>
<ol style="list-style-type: decimal">
<li>Starts at the head of the linked list and iterates along it until a link is found that has sufficient free space for the process</li>
<li>If the requested space is the same as the amount of free space in this partition, all the space is allocated</li>
<li>Else, the free link is split into two- the first node is set to the size requested and marked &quot;used&quot; whilst the second is set to the remaining size and marked &quot;free&quot;</li>
</ol>
<h4 id="next-fit-algorithm">Next Fit Algorithm</h4>
<ol style="list-style-type: decimal">
<li>Same as <strong>first-fit</strong> except stores where in the linked lists it stops, and then restarts the search from here after</li>
<li>Gives an even chance to all memory to get allocated while first fit concentrates on the start of the list. Remember the linear array analogy, imagine repeatedly starting from the bottom to find free space.</li>
</ol>
<p>Simulations have shown that <em>next fit</em> actually gives a worse performance than <em>first fit</em>.</p>
<h4 id="problems-with-firstnext-fit">Problems with First/Next Fit</h4>
<p>These methods are both very fast due to their YOLO nature, but this can also lead to some disadvantages. For example, first fit is only looking for the first available hole/gap. Once it finds it, it doesn't give two fucks whether or not there is a more suitable free partition later on. By then its already called &quot;gg ez&quot;. This is problem because there could potentially be a free partition later on which fits the process exactly. In this scenario, the algorithm is unneccessarily breaking up a big hole, which brings more problems. Next fit doesn't improve much on this front; and so we have to look at other potential algorithms.</p>
<h4 id="best-fit">Best Fit</h4>
<ul>
<li>The <strong>best fit</strong> algorithm will always search the entire linked list in order to find the smallest suitable hole for the memory request. As you can imagine, this is a lot slower than first/next fit.</li>
<li>Best fit is the same as first/next fit in the respect that there is no guarantee that a partition with the exact right amount of memory will exist. It uses the same splitting method to solve this problem. However, since best fit finds the smallest hole to split, there could be a lot of very small holes generated. These are useless bits of memory that barely any processes will be able to use.</li>
</ul>
<p>Note that it IS possible to merge free partitions that are next to each other and this is called <strong>Coalescing</strong> (more on that in a bit).</p>
<h4 id="worst-fit">Worst fit</h4>
<ul>
<li>Worst fit finds the largest available empty partition/hole and splits it. The idea being that there won't be any useless holes being left behind like there are with <em>best fit</em>. The holes will instead be large and probably more useful.</li>
<li>However in reality, this algorithm sucks ass too.</li>
</ul>
<p>He mentions in the lecture that a typical question in an exam would be to list the advantages and disadvantages of these methods. So make sure you understand them!</p>
<h4 id="quick-fit">Quick Fit</h4>
<ul>
<li>Maintains separate lists for commonly used sizes. For example could have lists for partitions of sizes 4k, 8k, 12k and so on.</li>
<li>This is a lot faster than <em>best fit</em> at finding a required sized partition.</li>
<li>Has same problem as <em>best fit</em> in that it can create tiny, useless partitions.</li>
<li><em>Coalescing</em> (merging free spaces) is difficult due to the fact that it uses multiple lists.</li>
</ul>
<h3 id="managing-available-memory">Managing Available Memory</h3>
<h4 id="coalescing">Coalescing</h4>
<p>Takes place when two <em>adjacent entries</em> in the linked list become free. When a block is freed, both neighbours are examined. Two or three entries are then combined into a larger block by adding up the sizes. The excess node(s) are deleted and the length/amount of memory for the node that was originally first is updated.</p>
<h4 id="compacting">Compacting</h4>
<p>As you can imagine, <em>coalescing</em> can only take you so far. If it gets to the point where lots of free blocks are being sandwiched by used blocks, and are distributed across memory, then we must use <strong>compacting</strong>. Obviously, this is a pretty time-consuming process - more so than coalescing. It is a three step process: 1. Process is swapped out 2. Free space is coalesced 3. Process swapped back in at lowest available location</p>
<p>This part of the lectured marked the end of the study of <em>contiguous</em> allocation schemes. Reminder that this is when processes are NOT split up to be stored in memory, but left as they are. The problems of such schemes are highlighted here. These include <em>internal fragmentation</em> from fixed-partitioning, as well as <em>external fragmentation</em> (wasted free holes) from dynamic partitioning.</p>
<h3 id="paging">Paging</h3>
<p>Paging is based on the principles of <em>fixed partitioning</em> and <em>code re-location</em>. Code relocation is the process of translating from logical/virtual memory to physical memory. Paging works by splitting memory into smaller blocks. One or more blocks are allocated to a process. For example, a 11KB process could take up 3 blocks of 4 KB. Internal fragmentation (excess, wasted memory) is reduced since there can only be excess memory wasted for the last block. There is also no external fragmentation since blocks are stacked directly onto each other in main memory (one of the main reasons for using a non-contiguous approach).</p>
</body>
</html>
